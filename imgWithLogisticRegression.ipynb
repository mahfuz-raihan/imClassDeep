{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612ceefa-5649-48cf-99f7-a7dad0de3934",
   "metadata": {},
   "source": [
    "# Image Classification with Logistic Regression using PyTroch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec339f76-5312-4b3e-8240-a6d37d08a92e",
   "metadata": {},
   "source": [
    "Here we use MNIST dataset to explore the data and most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29a289f-eeab-4dd1-a040-71314468e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libararies\n",
    "import torch as tr\n",
    "import torchvision as tv\n",
    "from torchvision.datasets import MNIST as MT\n",
    "import torchvision.transforms as tf\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb345f-09e3-4865-9343-cf54f073636c",
   "metadata": {},
   "source": [
    "### Download the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca05d6b-2321-4fe0-b898-3aa0f02de78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset download\n",
    "train_data = MT(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a61d8e4-4cc7-49a2-87cb-975fed161349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) # find the leanth of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8352917f-5088-4f81-9502-d4d98bd43445",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = MT(root='data/', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d7032e-30be-483c-add6-ebd7789471dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265bd13-feff-4cc1-95ce-3485bf3831a9",
   "metadata": {},
   "source": [
    "### Let's show the image from the dataset\n",
    "For this, we will use matplotlib for showcase the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3f2b9b-8bd7-49d8-b41b-28ed9c2f745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[4]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d12c03-b9a3-4b08-9fb2-20a7aef5695e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x1DAA1275610>, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e9a9b3-77c4-4cdf-b30a-3b72720d6e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c0ed60-ad96-40b5-ade8-37905d03af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MT(root='data/',\n",
    "             train=True,\n",
    "             transform=tf.ToTensor()\n",
    "             )\n",
    "test_ds = MT(root='data/',\n",
    "            train=False,\n",
    "            transform=tf.ToTensor()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec15ee18-885e-4f7f-980d-2cd00b8c93c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label= train_ds[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca1f1f-4917-4a76-942d-e0a75071483a",
   "metadata": {},
   "source": [
    "### let's look at some image sample values inside the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc38429-b711-47f9-8c0e-a6405fe28129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:, 10:15, 10:15])\n",
    "print(tr.max(img_tensor), tr.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e0b39c8-2b07-4987-99c8-a332da63e554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1daa13a3790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the image by passing in the 28x28 matrix\n",
    "plt.imshow(img_tensor[0, 10:20, 10:20], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb47b0-7039-4a80-b8a6-4bb4d84610fc",
   "metadata": {},
   "source": [
    "### Training and Validation Datasets\n",
    "we will randomly pick the images from the dataset, for this reason we import the random_split method from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0047d4a2-2d03-4464-b45f-4bc78e79a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(train_ds, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c0c6af-6b8e-4ec6-9584-33fb7ddec745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the lenth of the dataset\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0895795-3990-4185-a1af-83692933f883",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e824dc9b-3a79-4ede-a070-60bf5c2d79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataloader library\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ab87c5-3889-4566-b329-a9fcc354ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size= 100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4799dc7-2731-465d-8805-5cecb3be695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87e2ab05-05b3-41d8-952a-ba873b613c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0124, -0.0347, -0.0059,  ...,  0.0012, -0.0036,  0.0219],\n",
       "        [ 0.0279,  0.0133,  0.0045,  ...,  0.0143,  0.0007, -0.0002],\n",
       "        [ 0.0183,  0.0057,  0.0292,  ..., -0.0208,  0.0319,  0.0128],\n",
       "        ...,\n",
       "        [ 0.0266,  0.0333, -0.0209,  ..., -0.0213,  0.0202, -0.0213],\n",
       "        [-0.0356,  0.0341, -0.0274,  ..., -0.0080,  0.0051, -0.0249],\n",
       "        [-0.0215, -0.0295,  0.0244,  ..., -0.0115,  0.0162, -0.0317]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "800df9be-86da-474c-947c-c5f4a5128086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0128,  0.0268, -0.0333, -0.0046, -0.0118, -0.0265, -0.0223, -0.0095,\n",
       "         0.0214, -0.0075], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6f1a368-98cb-41f2-87e4-e122c3a5bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 7, 8, 9, 4, 9, 7, 6, 7, 8, 6, 1, 9, 3, 1, 0, 6, 7, 1, 2, 8, 9, 5, 3,\n",
      "        2, 7, 2, 5, 6, 5, 5, 9, 9, 3, 0, 2, 2, 9, 3, 1, 0, 5, 4, 1, 6, 9, 6, 2,\n",
      "        1, 1, 6, 1, 6, 3, 3, 1, 0, 9, 7, 0, 1, 5, 1, 7, 5, 3, 7, 8, 4, 2, 3, 0,\n",
      "        7, 2, 6, 0, 7, 3, 2, 7, 6, 2, 7, 9, 5, 2, 8, 4, 2, 3, 1, 0, 0, 6, 0, 8,\n",
      "        3, 6, 4, 6])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Now we will take first batch of 100 images from our dataset and pass the into our model\n",
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    #outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75c7c4b6-1eff-4448-882e-2b2914d793cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83956e50-8f50-48e8-8f52-a4414b432f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.reshape(100, 784).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a664ecf-d1a1-4780-b063-d9a185fa69e6",
   "metadata": {},
   "source": [
    "This error leads us for mismatch for image vector. our image are of the shape of 28x28 but we need them to be bector fo size 784 i.e. we need to flatten them out. We'll use  the ```.reshape``` method of a tensor, which will allow us to efficiently 'view' each image as a flat vector. \n",
    "Now we need to costom ```nn.Module``` class from pyTroch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be013db1-41f4-47f9-8ff5-122e80a73c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "\n",
    "model = mnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c16685c2-9177-4873-8d4b-c1c70509d63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9df6b776-fdce-4169-9627-70e6e3cb92a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0009,  0.0317,  0.0067,  ...,  0.0065, -0.0307,  0.0146],\n",
       "         [ 0.0278,  0.0114,  0.0292,  ..., -0.0004, -0.0260, -0.0228],\n",
       "         [-0.0095, -0.0188,  0.0108,  ...,  0.0349,  0.0122, -0.0347],\n",
       "         ...,\n",
       "         [ 0.0294, -0.0222, -0.0047,  ...,  0.0066, -0.0100, -0.0241],\n",
       "         [ 0.0256, -0.0013, -0.0194,  ..., -0.0003, -0.0224, -0.0030],\n",
       "         [ 0.0224,  0.0156,  0.0289,  ..., -0.0094, -0.0048,  0.0020]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0235,  0.0346,  0.0128, -0.0263,  0.0223,  0.0035, -0.0094,  0.0223,\n",
       "          0.0305, -0.0206], requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78eca61-1bf7-4b49-96ec-1048efa59f83",
   "metadata": {},
   "source": [
    "Our new custom model can be used in the exact same way as before. Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74f78b1c-121e-49ac-9112-f54ead7af627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs.shape:  torch.Size([100, 10])\n",
      "sample outputs: \n",
      " tensor([[-0.5142, -0.3682, -0.0162,  0.0483,  0.3800,  0.1586,  0.0014,  0.0470,\n",
      "         -0.0761, -0.2385],\n",
      "        [-0.0295, -0.1881,  0.2819, -0.2787,  0.2971,  0.1940,  0.0629,  0.1465,\n",
      "         -0.0934, -0.0548]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "print('Outputs.shape: ', outputs.shape)\n",
    "print(\"sample outputs: \\n\", outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976dea3-1db8-4757-9231-440b5440f02b",
   "metadata": {},
   "source": [
    "Each output row's elements must lie between 0 to 1 and add up to 1, which is not the case.\n",
    "To convert the output rows into probabilities, we use the softmax function, which has the following formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a9ce9f-e7f2-4090-ba32-6d27969d43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ecde0d3-0c76-4703-a789-c08e112f706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5142, -0.3682, -0.0162,  0.0483,  0.3800,  0.1586,  0.0014,  0.0470,\n",
       "         -0.0761, -0.2385],\n",
       "        [-0.0295, -0.1881,  0.2819, -0.2787,  0.2971,  0.1940,  0.0629,  0.1465,\n",
       "         -0.0934, -0.0548],\n",
       "        [-0.0780, -0.1339,  0.0502, -0.1732,  0.2498,  0.2555,  0.0472,  0.0739,\n",
       "          0.0606, -0.0891]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76b51160-f496-4854-8e23-191994790972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.0615, 0.0712, 0.1012, 0.1080, 0.1504, 0.1205, 0.1030, 0.1078, 0.0953,\n",
      "         0.0810],\n",
      "        [0.0923, 0.0787, 0.1260, 0.0719, 0.1279, 0.1154, 0.1012, 0.1100, 0.0866,\n",
      "         0.0900]])\n",
      "Sum:  0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax for each output row\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", tr.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3272fd42-8390-44c6-9aea-2c4a80aeea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 5, 4, 4, 5, 4, 8, 4, 4, 4, 7, 4, 6, 6, 4, 4, 8, 5, 4, 5, 4, 8, 4,\n",
      "        4, 7, 4, 4, 7, 4, 4, 7, 4, 4, 4, 8, 4, 4, 4, 6, 4, 8, 6, 4, 7, 4, 7, 4,\n",
      "        4, 4, 4, 8, 6, 4, 4, 4, 7, 7, 4, 5, 4, 4, 4, 4, 5, 8, 4, 6, 4, 2, 4, 5,\n",
      "        5, 4, 4, 4, 4, 7, 4, 2, 4, 7, 1, 7, 8, 7, 4, 1, 7, 4, 6, 5, 6, 4, 7, 4,\n",
      "        4, 4, 2, 5])\n",
      "tensor([0.1504, 0.1279, 0.1245, 0.1203, 0.1815, 0.1466, 0.1214, 0.1196, 0.1253,\n",
      "        0.1648, 0.1636, 0.1180, 0.1222, 0.1261, 0.1183, 0.1398, 0.1375, 0.1235,\n",
      "        0.1290, 0.1804, 0.1370, 0.1581, 0.1642, 0.1372, 0.1623, 0.1217, 0.1221,\n",
      "        0.1544, 0.1194, 0.1342, 0.1865, 0.1435, 0.1581, 0.1280, 0.1620, 0.1123,\n",
      "        0.1708, 0.1106, 0.1595, 0.1125, 0.1341, 0.1296, 0.1299, 0.1505, 0.1129,\n",
      "        0.1290, 0.1374, 0.1323, 0.1344, 0.1806, 0.1252, 0.1300, 0.1401, 0.1534,\n",
      "        0.1532, 0.1790, 0.1209, 0.1184, 0.1232, 0.1306, 0.1316, 0.1527, 0.1264,\n",
      "        0.1287, 0.1366, 0.1337, 0.1440, 0.1232, 0.1364, 0.1285, 0.1807, 0.1363,\n",
      "        0.1352, 0.1222, 0.1482, 0.1306, 0.1523, 0.1302, 0.1161, 0.1276, 0.1697,\n",
      "        0.1307, 0.1177, 0.1247, 0.1316, 0.1296, 0.1412, 0.1218, 0.1448, 0.1320,\n",
      "        0.1341, 0.1273, 0.1211, 0.1777, 0.1311, 0.1227, 0.1156, 0.1554, 0.1173,\n",
      "        0.1318], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = tr.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e82b6d4-850c-44b7-b7ee-43d656c34661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 7, 0, 0, 9, 6, 4, 9, 3, 5, 4, 3, 7, 8, 0, 0, 6, 9, 0, 9, 0, 0, 5,\n",
       "        8, 9, 4, 8, 5, 3, 2, 5, 2, 4, 0, 1, 6, 1, 2, 1, 3, 5, 9, 4, 9, 2, 9, 5,\n",
       "        9, 6, 3, 6, 7, 3, 3, 8, 7, 9, 9, 3, 0, 2, 2, 8, 7, 6, 0, 7, 6, 7, 8, 9,\n",
       "        1, 1, 0, 6, 3, 4, 1, 7, 0, 3, 3, 5, 6, 3, 8, 3, 7, 5, 7, 7, 9, 2, 4, 5,\n",
       "        1, 5, 4, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7cde8e-8254-497d-9d76-add639e0a205",
   "metadata": {},
   "source": [
    "## Evaluation Metric and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38bc7460-a72d-447b-bb62-46053722f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = tr.max(outputs, dim=1)\n",
    "    return tr.tensor(tr.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3e96298-51c4-45b2-818b-e2b145a5f38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.sum(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbe235b9-9fab-45d4-a726-5e1d9147dafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5142, -0.3682, -0.0162,  0.0483,  0.3800,  0.1586,  0.0014,  0.0470,\n",
       "         -0.0761, -0.2385],\n",
       "        [-0.0295, -0.1881,  0.2819, -0.2787,  0.2971,  0.1940,  0.0629,  0.1465,\n",
       "         -0.0934, -0.0548]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd0f4406-c196-423e-ac04-eee382d83315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0600)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57ee29a3-5a91-44d4-8007-3847a457308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3906, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# find the loss function\n",
    "# here we use cross-enropy as loss function\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "#loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de02c0-7da0-4c37-a4de-d006661cd1a2",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Here we will use the ```optim.SDG``` optimizer to update the weights and bias during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c907e852-0d8f-4b94-b6e2-b9f8a0d7ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "opt = tr.optim.SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ed140-62b3-4352-a33c-3e39bdc91978",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3326013-766f-412a-bcad-b2e28d5654d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt=tr.optim.SGD):\n",
    "    optimizer = opt(model.parameters(), lr)\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5164565-a0ee-4dee-a68b-1731bf8a4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f86db07-d1ca-4be8-a473-ba4bec383ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = tr.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = tr.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch+1, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf195fea-ebc2-4e08-be2c-bd1191a57605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.294098138809204, 'val_acc': 0.08290000259876251}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc2c90b7-9f7e-47e6-9836-fc4aeb47760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 1.8456, val_acc: 0.6672\n",
      "Epoch [2], val_loss: 1.5468, val_acc: 0.7320\n",
      "Epoch [3], val_loss: 1.3398, val_acc: 0.7621\n",
      "Epoch [4], val_loss: 1.1924, val_acc: 0.7840\n",
      "Epoch [5], val_loss: 1.0837, val_acc: 0.8017\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dd4d96b-f689-4a4c-b93b-7fe0fa467aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 1.0008, val_acc: 0.8118\n",
      "Epoch [2], val_loss: 0.9357, val_acc: 0.8190\n",
      "Epoch [3], val_loss: 0.8832, val_acc: 0.8260\n",
      "Epoch [4], val_loss: 0.8400, val_acc: 0.8303\n",
      "Epoch [5], val_loss: 0.8037, val_acc: 0.8343\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3985ee16-cd91-4292-9291-1250bf566bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.7728, val_acc: 0.8385\n",
      "Epoch [2], val_loss: 0.7462, val_acc: 0.8416\n",
      "Epoch [3], val_loss: 0.7230, val_acc: 0.8445\n",
      "Epoch [4], val_loss: 0.7025, val_acc: 0.8470\n",
      "Epoch [5], val_loss: 0.6843, val_acc: 0.8493\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76465a0-30df-412d-b5b2-5f63e978a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.6680, val_acc: 0.8504\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2e377-9d32-4d56-866a-5a26841077a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('acc vs no of epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b399a-5706-47cd-bd8d-2f2050c36c40",
   "metadata": {},
   "source": [
    "### **Apply on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48582d7d-d17b-4b02-a091-413334942994",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79487e14-0248-4cb4-bf20-641b9639707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = tr.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c1452-6068-48ac-8ece-6cccf4e59e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label,' , predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924e624-37b8-4bf6-8c4b-5d7870be235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[167]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label,' , predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e16cf-fc49-41e5-808e-96e7bf27118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[227]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label,' , predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04531102-7ce0-436b-8ef6-275e070ba252",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[560]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label,' , predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d65345-4139-42d3-9113-412bc2366d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[1982]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label,' , predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c16259-e1b9-42ab-9e8d-abf6dd70bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_ds[2678]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label,' , predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4842aac-7cd3-48c2-85b8-8bd67d28ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb56b8-e053-42ad-ab38-ab8ecaa04ea2",
   "metadata": {},
   "source": [
    "### Save and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07d5cb-7c53-42d3-b407-db30d79299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.save(model.state_dict(), 'mnist-logistic-regression.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cbc1e-4cda-4a65-b17a-9415febe301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe13633-1ecd-42ed-a1d8-fdd26c3614d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa192d-132f-47a8-8658-b4f200748084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ae7c3-7c17-468b-80d9-8a32065862c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model11, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c5803-2462-4b42-8580-b42a3b34c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.load_state_dict(tr.load('mnist-logistic-regression.pth'))\n",
    "model11.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7919ec-3233-4757-a4c2-f4789debde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=256)\n",
    "result = evaluate(model11, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cbc3f-2c45-4b76-92dd-cc2ec2df9788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
